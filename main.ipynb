{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec9c12c",
   "metadata": {},
   "source": [
    "# Topicfy your News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3b74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import newspaper\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d01243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_scraper(url, articles):\n",
    "    '''\n",
    "    From the given URL, add news articles into the articles list\n",
    "    '''\n",
    "    # get the RSS feed and turn it into a usable object\n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    # add each article in the feed into articles list\n",
    "    for entry in feed.entries:\n",
    "        # obtain full article of the entry\n",
    "        article = newspaper.Article(entry.link)\n",
    "\n",
    "        # fetches the HTML and convert it into readable object\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        # store article contents as a dictionary within articles list\n",
    "        articles.append({\n",
    "            'title': article.title,\n",
    "            'author': article.authors,\n",
    "            'publish_date': article.publish_date,\n",
    "            'content': article.text\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 1: (keywords: cent, 20, credit ap, ap, australian)\n",
      "- ASX opens higher, CBA slides\n",
      "\n",
      "Cluster 2: (keywords: sorry feature, currently, dismiss, working restore, sorry)\n",
      "- Ex-childcare worker charged with child sex offences\n",
      "- Two people rescued after going overboard on Disney cruise ship\n",
      "- Second truck in two days slams into notorious Melbourne bridge\n",
      "- Teen gun reflects on 'crazy' Wimbledon warm-up win\n",
      "- Designer reveals impact of dress Princess Diana chose not to wear\n",
      "- Slater explains Walsh snub\n",
      "- Curfew forces match to end amid boos\n",
      "- Thousands of 'love bugs' inundate parts of South Korea\n",
      "\n",
      "Cluster 3: (keywords: prison, family, freedom, financial, financial freedom)\n",
      "- I’m chasing financial freedom. How do I escape the rat race?\n",
      "- WA news LIVE: Findings in Cleveland Dodd inquest expected today\n",
      "\n",
      "Cluster 4: (keywords: centre, said, fashion, theatre, paris)\n",
      "- The unlikely fashion collaboration that will be a summer blockbuster\n",
      "- ‘Everybody’s shot up here’: Suspected gunman in US firefighter ambush killings named\n",
      "- Melbourne childcare worker charged with more than 70 offences, including child rape\n",
      "- The next Enmore Theatre is not where you expect\n",
      "- ‘Southgate is falling apart’: Riverside restaurant calls time after 21 years\n",
      "- Port guide: Paris, France\n",
      "\n",
      "Cluster 5: (keywords: weather, flights, iran, nsw, said)\n",
      "- Australia news LIVE: Anthony Albanese brushes off claims Trump insulted him; RBA expected to slash rates; Trump to host Netanyahu for talks next Monday as US presses for Gaza ceasefire\n",
      "- Brisbane news live: Flights cancelled amid severe weather | Woman pinned by car airlifted to hospital | Child safety inquiry gets to work\n",
      "- Sydney weather LIVE updates: ‘Bomb cyclone’ event expected across NSW eastern coastline as BOM forecasts a month’s worth of rain in six hours; SES warns storm impact will be ‘short but intense’\n"
     ]
    }
   ],
   "source": [
    "def produce_cluster(contents, n_clusters):\n",
    "    '''\n",
    "    perform k-mean clustering using TF-IDF vectorisation on the contents of the new articles\n",
    "    '''\n",
    "    # clean the the contents\n",
    "    contents\n",
    "\n",
    "    # convert text into vector of numeric values, keeping the top 2000 words, includes unigrams and bigrams\n",
    "    vectoriser = TfidfVectorizer(max_features=100, ngram_range=(1, 2), stop_words='english')\n",
    "    # sparse matrix with articles as rows, word(s) as columns, importance of each word(s) as the value\n",
    "    X = vectoriser.fit_transform(contents)\n",
    "\n",
    "    # use kmean clustering based on the results of the TF-IDF vectors\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    return model.labels_, model, vectoriser\n",
    "\n",
    "def get_top_keywords_per_cluster(model, vectoriser, top_n):\n",
    "    '''\n",
    "    extracts the top top_n-th most common key word for each cluster\n",
    "    '''\n",
    "    # extract all word and phrases used by the vectoriser\n",
    "    terms = vectoriser.get_feature_names_out()\n",
    "    # initiate list containing key words of each cluster \n",
    "    keywords_per_cluster = []\n",
    "\n",
    "    for i in range(model.n_clusters):\n",
    "        # get the average term weights in the cluster\n",
    "        center = model.cluster_centers_[i]\n",
    "        # get the top terms based on indices\n",
    "        top_indices = center.argsort()[-top_n:][::-1]\n",
    "        top_terms = [terms[j] for j in top_indices]\n",
    "        keywords_per_cluster.append(top_terms)\n",
    "        \n",
    "    return keywords_per_cluster\n",
    "\n",
    "\n",
    "# Your feed URL\n",
    "feed_url = 'https://www.smh.com.au/rss/feed.xml'\n",
    "articles = []\n",
    "article_scraper(feed_url, articles)\n",
    "\n",
    "titles = [article['title'] for article in articles if article['title']]\n",
    "contents = [article['content'] for article in articles if article['content']]\n",
    "\n",
    "labels, model, vectoriser = produce_cluster(contents, n_clusters=5)\n",
    "\n",
    "top_keywords = get_top_keywords_per_cluster(model, vectoriser, top_n=5)\n",
    "\n",
    "for cluster_num in range(model.n_clusters):\n",
    "    print(f\"\\nCluster {cluster_num+1}: (keywords: {', '.join(top_keywords[cluster_num])})\")\n",
    "    for i, title in enumerate(titles):\n",
    "        if labels[i] == cluster_num:\n",
    "            print(f\"- {title}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
